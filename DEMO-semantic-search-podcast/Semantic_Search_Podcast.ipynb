{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_hyperlink_cell"
      },
      "source": [
        "[Go to Home Page](https://weaviate.oneblink.ai)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Search on Podcast Transcripts\n",
        "\n",
        "In this project, we will be using Weaviate to perform semantic search on podcast transcripts. We will utilize the OpenAI text2vec transformer module to vectorize the text, enabling us to conduct semantic searches on the data.\n",
        "\n",
        "The project's origin is [here](https://github.com/weaviate/weaviate-examples/tree/main/podcast-semantic). More information on the vectorization module can be found [here](https://weaviate.io/developers/weaviate/current/retriever-vectorizer-modules/text2vec-transformers.html#pre-built-images).\n",
        "\n",
        "The dataset consists of 300 podcast transcripts from [Changelog](https://github.com/thechangelog/transcripts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import weaviate\n",
        "from weaviate import Config\n",
        "import weaviate.classes as wvc\n",
        "import json\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from tqdm import tqdm\n",
        "import helper\n",
        "\n",
        "\n",
        "\n",
        "client = weaviate.connect_to_local(\n",
        "    port=8083,\n",
        "    grpc_port=50053)\n",
        "\n",
        "print(client.collections.delete(\"Podcast\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's create a new collection named 'Podcast' to store our podcast data, and check if the collection is created successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a new collection named Podcast\n",
        "client.collections.create(\n",
        "    name=\"Podcast\",\n",
        "    properties=[\n",
        "        wvc.Property(\n",
        "            name=\"title\",\n",
        "            data_type=wvc.DataType.TEXT,\n",
        "        ),\n",
        "        wvc.Property(\n",
        "            name=\"transcript\",\n",
        "            data_type=wvc.DataType.TEXT,\n",
        "        )\n",
        "    ],\n",
        "    vectorizer_config=wvc.Configure.Vectorizer.text2vec_transformers()\n",
        "\n",
        ")\n",
        "\n",
        "# Checking if the collection is created successfully\n",
        "print(client.collections.exists(\"Podcast\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will read our podcast dataset and start processing the transcripts. We will chunk the data as the transcripts are long and chunking will enhance the performance for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./data/podcast_ds.json\", 'r', encoding = 'utf-8') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "podcast = client.collections.get(\"Podcast\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")\n",
        "\n",
        "transcripts_to_add = []\n",
        "\n",
        "# Chunking the data as the transcripts are too long and won't give good performance for semantic search\n",
        "with helper.std_out_err_redirect_tqdm() as orig_stdout:\n",
        "    for data in tqdm(datastore, desc='Importing transcripts', file = orig_stdout, unit = 'transcript'):\n",
        "        transcripts_to_add = []\n",
        "        chunked_transcript = text_splitter.create_documents([data[\"transcript\"]])\n",
        "        for chunk in chunked_transcript:\n",
        "            transcripts_to_add.append(\n",
        "                wvc.DataObject(\n",
        "                    properties={\n",
        "                        \"title\": data[\"title\"],\n",
        "                        \"transcript\": chunk.page_content,\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "        response = podcast.data.insert_many(transcripts_to_add)\n",
        "        message = str(data[\"title\"]) + ' imported'\n",
        "        helper.log(message)\n",
        "        print(response.errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's verify by fetching a few objects from our 'Podcast' collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(podcast.query.fetch_objects(limit=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_hyperlink_cell"
      },
      "source": [
        "[Go to Home Page](https://weaviate.oneblink.ai)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}