{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_hyperlink_cell"
      },
      "source": [
        "[Go to Home Page](/weaviate/lab/workspaces/auto-3/tree/main.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00efd7c2",
      "metadata": {},
      "source": [
        "# Bedrock Model Invocation\n",
        "This notebook contains a Python script to invoke various Bedrock models. The script defines functions to select and interact with different AI models like COHERE, TITAN, CLAUDE, and others, using the Bedrock API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "# Listing Bedrock Models\n",
        "Your SageMaker notebook comes with full access to all Amazon Bedrock Models. In addition, an IAM role has been associated with the notebook to allow seamless invocation without having to pass AWS credentials explicitly.\n",
        "\n",
        "Follow the steps below to list all available Bedrock models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "list_models",
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "\n",
        "def list_bedrock_models():\n",
        "    try:\n",
        "        client = boto3.client('bedrock')\n",
        "        response = client.list_foundation_models()\n",
        "        \n",
        "        # Prepare data for pretty display\n",
        "        model_data = []\n",
        "        for model in response['modelSummaries']:\n",
        "            model_data.append({\n",
        "                'Model Name': model['modelName'],\n",
        "                'Provider Name': model['providerName']\n",
        "            })\n",
        "        \n",
        "        # Convert to DataFrame for pretty display\n",
        "        df = pd.DataFrame(model_data)\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error listing models: {str(e)}\")\n",
        "\n",
        "# Display the table\n",
        "df = list_bedrock_models()\n",
        "if df is not None:\n",
        "    from IPython.display import display\n",
        "    pd.set_option('display.max_rows', None)  # Set to display all rows\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00efd7c3",
      "metadata": {},
      "source": [
        "# Invoking Bedrock Models\n",
        "Next, you can invoke different Bedrock models using the provided function. The function `invoke_bedrock_model` allows you to specify the model type and various parameters for invocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "invoke_model",
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "def invoke_bedrock_model(model_type, template, max_tokens, temperature, top_p, stop_sequences):\n",
        "    \"\"\"\n",
        "    Invoke a Bedrock model based on the given model type and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    - model_type (str): The type of the model (e.g., \"TITAN\", \"CLAUDE\", etc.)\n",
        "    - template (str): The prompt for the model.\n",
        "    - max_tokens (int): Maximum number of tokens to sample.\n",
        "    - temperature (float): Sampling temperature.\n",
        "    - top_p (float): Top-p sampling value.\n",
        "    - stop_sequences (list): List of sequences to stop the sampling.\n",
        "\n",
        "    Returns:\n",
        "    - None: Prints the model's completion.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine the model ID based on the provided model type\n",
        "    if model_type == \"TITAN\":\n",
        "        model_id = \"amazon.titan-tg1-large\"\n",
        "    elif model_type == \"CLAUDE\":\n",
        "        model_id = \"anthropic.claude-v1\"\n",
        "    elif model_type == \"CLAUDE-INSTANT\":\n",
        "        model_id = \"anthropic.claude-instant-v1\"\n",
        "    elif model_type == \"J2\":\n",
        "        model_id = \"ai21.j2-jumbo-instruct\"\n",
        "    else:\n",
        "        # Handle unsupported models\n",
        "        raise Exception(\"Unsupported model\")\n",
        "\n",
        "    # Initialize the Bedrock client\n",
        "    bedrock = boto3.client('bedrock-runtime')\n",
        "\n",
        "    # Prepare the request body with the parameters\n",
        "    body = {\n",
        "        \"prompt\": template,\n",
        "        \"max_tokens_to_sample\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"stop_sequences\": stop_sequences\n",
        "    }\n",
        "    body_string = json.dumps(body)\n",
        "\n",
        "    # Invoke the Bedrock model and get the response\n",
        "    response = bedrock.invoke_model(\n",
        "        modelId=model_id,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        body=body_string)\n",
        "\n",
        "    # Parse the response and print the completion\n",
        "    json_obj = json.loads(response.get(\"body\").read().decode())\n",
        "    print(json_obj['completion'])\n",
        "\n",
        "\n",
        "def bedrock_smoke_test():\n",
        "    # Example usage of the invoke_bedrock_model function\n",
        "    model_type=\"CLAUDE-INSTANT\"\n",
        "    prompt = \"What does Weaviate do? What are the top use cases? How can Amazon Bedrock be used with Weaviate?\"\n",
        "    template = f\"\"\"Human: {prompt}\n",
        "                    Assistant:\"\"\"\n",
        "    max_tokens = 5000\n",
        "    stop_sequences=[]\n",
        "    temperature=0\n",
        "    top_p=0.9\n",
        "    invoke_bedrock_model(model_type, template, max_tokens, temperature, top_p, stop_sequences)\n",
        "\n",
        "# Run the smoke test when the script is executed\n",
        "bedrock_smoke_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344d186a-04c4-4ff0-9eb4-24fd63e58c32",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef0085f-98b6-437e-a01b-8979c71849cc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_hyperlink_cell"
      },
      "source": [
        "[Go to Home Page](/weaviate/lab/workspaces/auto-3/tree/main.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
